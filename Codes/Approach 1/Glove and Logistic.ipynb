{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove Embedding + Logistic Regression (cammy code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.093112Z",
     "start_time": "2019-11-22T05:55:03.000347Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm #display progress bar\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import punkt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.721476Z",
     "start_time": "2019-11-22T05:55:05.096732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50960, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.749400Z",
     "start_time": "2019-11-22T05:55:05.724467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>clean</th>\n",
       "      <th>cleaned_comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2f52adcf5a111cd3</td>\n",
       "      <td>That is about it for for now. Primarily, I wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>primarily work citations either add update rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>819b3339c747286f</td>\n",
       "      <td>\"\\n I wasn't aware that peer-reviewed studies ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>aware peerreviewed study minimal methodologica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b66e5fffbd70f8fe</td>\n",
       "      <td>\"\\nIt's fine to edit for personal gain so long...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fine edit personal gain long edit accord basic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fd7f2ec6efe0315d</td>\n",
       "      <td>I did not add these words to the PLANS website...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>add word plan website contact plan plan skepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>f78b624060552c1a</td>\n",
       "      <td>\"\\n\\n List of recent changes \\n\\nRequested by ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>list recent change request sarge baldy even th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  2f52adcf5a111cd3  That is about it for for now. Primarily, I wor...      0   \n",
       "1  819b3339c747286f  \"\\n I wasn't aware that peer-reviewed studies ...      0   \n",
       "2  b66e5fffbd70f8fe  \"\\nIt's fine to edit for personal gain so long...      0   \n",
       "3  fd7f2ec6efe0315d  I did not add these words to the PLANS website...      0   \n",
       "4  f78b624060552c1a  \"\\n\\n List of recent changes \\n\\nRequested by ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  clean  \\\n",
       "0             0        0       0       0              0      1   \n",
       "1             0        0       0       0              0      1   \n",
       "2             0        0       0       0              0      1   \n",
       "3             0        0       0       0              0      1   \n",
       "4             0        0       0       0              0      1   \n",
       "\n",
       "                                cleaned_comment_text  \n",
       "0  primarily work citations either add update rel...  \n",
       "1  aware peerreviewed study minimal methodologica...  \n",
       "2  fine edit personal gain long edit accord basic...  \n",
       "3  add word plan website contact plan plan skepti...  \n",
       "4  list recent change request sarge baldy even th...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.768352Z",
     "start_time": "2019-11-22T05:55:05.752395Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create X and y dataframe\n",
    "X = df[\"cleaned_comment_text\"]\n",
    "y = df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.790291Z",
     "start_time": "2019-11-22T05:55:05.773336Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    primarily work citations either add update rel...\n",
       "1    aware peerreviewed study minimal methodologica...\n",
       "2    fine edit personal gain long edit accord basic...\n",
       "3    add word plan website contact plan plan skepti...\n",
       "4    list recent change request sarge baldy even th...\n",
       "Name: cleaned_comment_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.811235Z",
     "start_time": "2019-11-22T05:55:05.793283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.838163Z",
     "start_time": "2019-11-22T05:55:05.816222Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.850132Z",
     "start_time": "2019-11-22T05:55:05.842153Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30200                              perhaps perhaps che pig\n",
       "34498    three women elgar life watch min tv documentar...\n",
       "25278    edit infobox add audio p nks performance live ...\n",
       "29073    weird indeed put four tildes keep come unsigne...\n",
       "9290                               contrary recommendation\n",
       "Name: cleaned_comment_text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.868084Z",
     "start_time": "2019-11-22T05:55:05.853123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15420    look good change source talk theme musical sty...\n",
       "19180    really dog photo dog steal post page nastered ...\n",
       "46897                     listen children night music make\n",
       "4110            thank thank redirect link idk already page\n",
       "23094                                    tribe war muslims\n",
       "Name: cleaned_comment_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:55:05.879055Z",
     "start_time": "2019-11-22T05:55:05.871075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T06:00:23.761999Z",
     "start_time": "2019-11-22T05:55:05.882045Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [05:17, 6908.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195885 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 3min Found 2195885 word vectors.\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "       coefs = np.asarray(values[1:], dtype='float32')\n",
    "       embeddings_index[word] = coefs\n",
    "    except ValueError:\n",
    "       pass\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T06:00:23.778956Z",
     "start_time": "2019-11-22T06:00:23.763972Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    # remove stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # remove non-alphabets\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T06:00:53.572787Z",
     "start_time": "2019-11-22T06:00:23.781919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 40768/40768 [00:24<00:00, 1678.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10192/10192 [00:05<00:00, 1907.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Vector for Sentences are created\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 17s create sentence vectors using the above function for training and test set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(X_test)]\n",
    "\n",
    "print('Normalized Vector for Sentences are created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T06:00:53.697307Z",
     "start_time": "2019-11-22T06:00:53.572787Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xtest_glove = np.array(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T06:01:00.684702Z",
     "start_time": "2019-11-22T06:00:53.699305Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 92.3%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9203,    8],\n",
       "       [ 777,  204]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      9211\n",
      "           1       0.96      0.21      0.34       981\n",
      "\n",
      "    accuracy                           0.92     10192\n",
      "   macro avg       0.94      0.60      0.65     10192\n",
      "weighted avg       0.93      0.92      0.90     10192\n",
      "\n",
      "Precision: 96.23%\n",
      "Recall: 20.8%\n",
      "F1 score: 34.2%\n",
      "=============================================================\n",
      "... Processing severe_toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 99.07%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10097,     0],\n",
       "       [   95,     0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10097\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.99     10192\n",
      "   macro avg       0.50      0.50      0.50     10192\n",
      "weighted avg       0.98      0.99      0.99     10192\n",
      "\n",
      "Precision: 96.23%\n",
      "Recall: 18.96%\n",
      "F1 score: 31.68%\n",
      "=============================================================\n",
      "... Processing obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 95.28%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9639,    4],\n",
       "       [ 477,   72]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      9643\n",
      "           1       0.95      0.13      0.23       549\n",
      "\n",
      "    accuracy                           0.95     10192\n",
      "   macro avg       0.95      0.57      0.60     10192\n",
      "weighted avg       0.95      0.95      0.94     10192\n",
      "\n",
      "Precision: 95.83%\n",
      "Recall: 16.98%\n",
      "F1 score: 28.86%\n",
      "=============================================================\n",
      "... Processing threat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 99.69%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10160,     0],\n",
       "       [   32,     0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     10160\n",
      "           1       0.00      0.00      0.00        32\n",
      "\n",
      "    accuracy                           1.00     10192\n",
      "   macro avg       0.50      0.50      0.50     10192\n",
      "weighted avg       0.99      1.00      1.00     10192\n",
      "\n",
      "Precision: 95.83%\n",
      "Recall: 16.66%\n",
      "F1 score: 28.38%\n",
      "=============================================================\n",
      "... Processing insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 95.38%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9673,    5],\n",
       "       [ 466,   48]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      9678\n",
      "           1       0.91      0.09      0.17       514\n",
      "\n",
      "    accuracy                           0.95     10192\n",
      "   macro avg       0.93      0.55      0.57     10192\n",
      "weighted avg       0.95      0.95      0.94     10192\n",
      "\n",
      "Precision: 95.01%\n",
      "Recall: 14.92%\n",
      "F1 score: 25.8%\n",
      "=============================================================\n",
      "... Processing identity_hate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 99.04%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10094,     0],\n",
       "       [   98,     0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cammy\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     10094\n",
      "           1       0.00      0.00      0.00        98\n",
      "\n",
      "    accuracy                           0.99     10192\n",
      "   macro avg       0.50      0.50      0.50     10192\n",
      "weighted avg       0.98      0.99      0.99     10192\n",
      "\n",
      "Precision: 95.01%\n",
      "Recall: 14.28%\n",
      "F1 score: 24.83%\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "classifier = LogisticRegression(C=0.01)\n",
    "\n",
    "TP = list()\n",
    "FP = list()\n",
    "FN = list()\n",
    "TN = list()\n",
    "accuracy = list()\n",
    "\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    \n",
    "    # train the model\n",
    "    classifier.fit(xtrain_glove, y_train[category])\n",
    "    \n",
    "    # compute the testing accuracy\n",
    "    prediction = classifier.predict_proba(xtest_glove)[:, 1]\n",
    "    # round() because we use predict_proba here\n",
    "    print('Test accuracy is {}%'.format(round(100*accuracy_score(y_test[category], prediction.round()), 2)))\n",
    "    display(confusion_matrix(y_test[category], prediction.round()))\n",
    "    print(classification_report(y_test[category], prediction.round()))\n",
    "    \n",
    "    # getting precision, recall and F1-score    \n",
    "    cm = confusion_matrix(y_test[category], prediction.round())\n",
    "    TN_i = cm[0][0]\n",
    "    FP_i = cm[0][1]\n",
    "    FN_i = cm[1][0]\n",
    "    TP_i = cm[1][1]\n",
    "    \n",
    "    TP.append(TP_i)\n",
    "    FP.append(FP_i)\n",
    "    FN.append(FN_i)\n",
    "    TN.append(TN_i)\n",
    "    \n",
    "    precision = sum(TP) / (sum(TP) + sum(FP))\n",
    "    recall = sum(TP) / (sum(TP) + sum(FN))\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"Precision: {}%\".format(np.round(100*precision, 2)))\n",
    "    print(\"Recall: {}%\".format(np.round(100*recall, 2)))\n",
    "    print(\"F1 score: {}%\".format(np.round(100*f1_score, 2))) \n",
    "    print(\"=============================================================\")\n",
    "    \n",
    "    accuracy_i = (TP_i + TN_i) / (TP_i + TN_i + FN_i + FP_i)\n",
    "    accuracy.append(accuracy_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T06:01:00.716009Z",
     "start_time": "2019-11-22T06:01:00.684702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 95.01%\n",
      "Micro Recall: 14.28%\n",
      "Micro F1-score: 24.83%\n",
      "Average Accuracy: 96.79%\n"
     ]
    }
   ],
   "source": [
    "precision = list()\n",
    "recall = list()\n",
    "f1_score = list()\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "no_of_labels = 6\n",
    "\n",
    "for i in range(no_of_labels):\n",
    "    precision_i = (TP[i]) / ((TP[i] + FP[i]) or not (TP[i] + FP[i]))\n",
    "    precision.append(precision_i)\n",
    "\n",
    "    recall_i = (TP[i]) / ((TP[i] + FN[i]) or not (TP[i] + FN[i]))\n",
    "    recall.append(recall_i)\n",
    "\n",
    "    f1_score_i = (2 * precision_i * recall_i) / ((precision_i + recall_i) or not (precision_i + recall_i))\n",
    "    f1_score.append(f1_score_i)\n",
    "\n",
    "micro_precision = sum(TP) / (sum(TP) + sum(FP))\n",
    "micro_recall = sum(TP) / (sum(TP) + sum(FN))\n",
    "micro_f1_score = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "avg_accuracy = sum(accuracy) / no_of_labels\n",
    "\n",
    "print(\"Micro Precision: {}%\".format(round(100*micro_precision, 2)))\n",
    "print(\"Micro Recall: {}%\".format(round(100*micro_recall, 2)))\n",
    "print(\"Micro F1-score: {}%\".format(round(100*micro_f1_score, 2)))\n",
    "print(\"Average Accuracy: {}%\".format(round(100*avg_accuracy, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
